"""CLI entry point for speaker-aware transcription."""

import json
import os
import shutil
import sys
from pathlib import Path
from typing import Optional

import click
from dotenv import dotenv_values

from .database import Database
from .env import load_transcribe_env
from .models import DiarizedSegment
from .paths import get_data_dir, get_env_path
from .timezone_utils import format_local_time
from .web.config import get_settings


@click.group()
@click.option(
    "--db",
    type=click.Path(),
    default=None,
    help="Path to database file (default: ~/.transcribe_local/speakers.db)",
)
@click.pass_context
def cli(ctx: click.Context, db: Optional[str]) -> None:
    """Speaker-aware local transcription CLI."""
    ctx.ensure_object(dict)
    db_path = Path(db) if db else None
    ctx.obj["db"] = Database(db_path)


def _prompt_optional_secret(label: str, currently_set: bool) -> Optional[str]:
    """Prompt for a secret value and allow skipping."""
    if not click.confirm(
        f"Configure {label}? (currently {'set' if currently_set else 'not set'})",
        default=not currently_set,
    ):
        return None

    value = click.prompt(
        label,
        default="",
        show_default=False,
        hide_input=True,
    ).strip()
    return value or None


def _write_env_file(env_path: Path, openai_key: Optional[str], hf_token: Optional[str]) -> None:
    """Write transcribe-local credentials env file."""
    env_path.parent.mkdir(parents=True, exist_ok=True)
    lines = [
        "# Transcribe Local environment file",
        "# Generated by: transcribe-local setup",
        "",
        f"OPENAI_API_KEY={openai_key or ''}",
        f"HF_TOKEN={hf_token or ''}",
        "",
    ]
    env_path.write_text("\n".join(lines), encoding="utf-8")


def _looks_like_url(value: str) -> bool:
    return value.startswith("http://") or value.startswith("https://")


def _format_status(ok: bool) -> str:
    return "[OK]" if ok else "[FAIL]"


@cli.command()
@click.option("--non-interactive", is_flag=True, help="Use defaults/flags and skip prompts")
@click.option("--mode", type=click.Choice(["hybrid", "openai", "local"]), default=None, help="Default transcription mode")
@click.option("--server", default=None, help="Default uploader server URL")
@click.option("--language", default=None, help="Default language code (leave unset for auto-detect)")
@click.option("--openai-key", default=None, help="Set OPENAI_API_KEY")
@click.option("--hf-token", default=None, help="Set HF_TOKEN")
def setup(
    non_interactive: bool,
    mode: Optional[str],
    server: Optional[str],
    language: Optional[str],
    openai_key: Optional[str],
    hf_token: Optional[str],
) -> None:
    """Interactive setup wizard for first-time configuration."""
    settings = get_settings()
    data_dir = get_data_dir()
    env_path = get_env_path()

    data_dir.mkdir(parents=True, exist_ok=True)
    existing_env = dotenv_values(env_path) if env_path.exists() else {}

    click.echo("Transcribe Local setup")
    click.echo(f"Data directory: {data_dir}")
    click.echo(f"Config file: {settings.config_path}")
    click.echo(f"Env file: {env_path}")
    click.echo("")

    # Defaults from existing settings
    selected_mode = mode or settings.transcription_mode
    selected_server = server or settings.server_url
    selected_language = language if language is not None else settings.default_language
    selected_openai = openai_key if openai_key is not None else existing_env.get("OPENAI_API_KEY")
    selected_hf = hf_token if hf_token is not None else existing_env.get("HF_TOKEN")

    if not non_interactive:
        selected_mode = click.prompt(
            "Default transcription mode",
            type=click.Choice(["hybrid", "openai", "local"]),
            default=selected_mode,
            show_default=True,
        )

        selected_server = click.prompt(
            "Default uploader server URL",
            default=selected_server,
            show_default=True,
        ).strip()

        selected_language_raw = click.prompt(
            "Default language code (empty = auto-detect)",
            default=selected_language or "",
            show_default=False,
        ).strip()
        selected_language = selected_language_raw or None

        maybe_openai = _prompt_optional_secret("OPENAI_API_KEY", bool(selected_openai))
        if maybe_openai is not None:
            selected_openai = maybe_openai

        maybe_hf = _prompt_optional_secret("HF_TOKEN", bool(selected_hf))
        if maybe_hf is not None:
            selected_hf = maybe_hf

    if selected_server and not _looks_like_url(selected_server):
        click.echo("Error: --server must start with http:// or https://", err=True)
        sys.exit(1)

    # Persist settings
    settings.transcription_mode = selected_mode
    settings.server_url = selected_server
    settings.default_language = selected_language
    settings.save()

    # Ensure expected data folders exist
    for p in (
        data_dir / "recordings",
        data_dir / "ready",
        data_dir / "uploaded",
        data_dir / "uploads",
        data_dir / "processed",
    ):
        p.mkdir(parents=True, exist_ok=True)

    _write_env_file(env_path, selected_openai, selected_hf)

    click.echo("")
    click.echo("Configuration saved.")
    click.echo(f"- mode: {settings.transcription_mode}")
    click.echo(f"- server_url: {settings.server_url}")
    click.echo(f"- default_language: {settings.default_language or 'auto'}")
    click.echo("")
    click.echo("Next steps:")
    click.echo("1) Run: transcribe-local doctor")
    click.echo("2) Run: transcribe-local jobs start")
    click.echo("3) Run: transcribe-local serve --host 0.0.0.0 --port 8000")


@cli.command()
@click.option("--check-server/--no-check-server", default=True, help="Probe configured server URL")
def doctor(check_server: bool) -> None:
    """Validate local setup and configuration."""
    import platform

    settings = get_settings()
    data_dir = get_data_dir()
    env_path = get_env_path()
    load_transcribe_env()

    checks: list[tuple[bool, str]] = []

    py_ok = sys.version_info >= (3, 10)
    checks.append((py_ok, f"Python {platform.python_version()} (requires >= 3.10)"))

    ffmpeg_ok = shutil.which("ffmpeg") is not None
    checks.append((ffmpeg_ok, "ffmpeg is installed"))

    ffprobe_ok = shutil.which("ffprobe") is not None
    checks.append((ffprobe_ok, "ffprobe is installed"))

    checks.append((data_dir.exists(), f"data dir exists: {data_dir}"))
    checks.append((settings.config_path.exists(), f"config exists: {settings.config_path}"))
    checks.append((env_path.exists(), f"env file exists: {env_path}"))

    openai_key = os.environ.get("OPENAI_API_KEY")
    hf_token = os.environ.get("HF_TOKEN")

    needs_openai = settings.transcription_mode in {"hybrid", "openai"}
    needs_hf = settings.transcription_mode in {"hybrid", "local"}

    checks.append((bool(openai_key) or not needs_openai, "OPENAI_API_KEY is configured"))
    checks.append((bool(hf_token) or not needs_hf, "HF_TOKEN is configured"))
    checks.append((_looks_like_url(settings.server_url), f"server_url looks valid: {settings.server_url}"))

    if check_server and _looks_like_url(settings.server_url):
        try:
            import requests

            resp = requests.get(f"{settings.server_url.rstrip('/')}/api/jobs", timeout=3)
            checks.append((resp.status_code == 200, f"server reachable at {settings.server_url}"))
        except Exception:
            checks.append((False, f"server reachable at {settings.server_url}"))

    click.echo("Transcribe Local diagnostics")
    failed = 0
    for ok, message in checks:
        click.echo(f"{_format_status(ok)} {message}")
        if not ok:
            failed += 1

    if failed == 0:
        click.echo("All checks passed.")
    else:
        click.echo(f"{failed} check(s) failed. Run `transcribe-local setup` to fix config.")


@cli.command()
@click.argument("audio_file", type=click.Path(exists=True))
@click.option("-l", "--language", default=None, help="Language code (auto-detect if omitted)")
@click.option("--min-speakers", type=int, default=None, help="Minimum expected speakers")
@click.option("--max-speakers", type=int, default=None, help="Maximum expected speakers")
@click.option("--threshold", type=float, default=0.5, help="Speaker match threshold (default: 0.5)")
@click.option("--no-match", is_flag=True, help="Skip automatic speaker matching")
@click.option("-o", "--output", type=click.Path(), default=None, help="Output file path")
@click.option("--format", "output_format", type=click.Choice(["txt", "json", "srt", "vtt"]), default="txt", help="Output format")
@click.option("--device", type=click.Choice(["auto", "cpu", "mps", "cuda"]), default="auto", help="Compute device")
@click.option("--model", default="large-v3", help="Whisper model to use")
@click.option("--openai", "use_openai", is_flag=True, help="Use OpenAI API instead of local models (no speaker diarization)")
@click.option("--hybrid", "use_hybrid", is_flag=True, help="Use OpenAI for transcription + local pyannote for diarization (recommended)")
@click.pass_context
def transcribe(
    ctx: click.Context,
    audio_file: str,
    language: Optional[str],
    min_speakers: Optional[int],
    max_speakers: Optional[int],
    threshold: float,
    no_match: bool,
    output: Optional[str],
    output_format: str,
    device: str,
    model: str,
    use_openai: bool,
    use_hybrid: bool,
) -> None:
    """Transcribe an audio file with speaker diarization.

    AUDIO_FILE: Path to the audio file to transcribe.

    Modes:
      (default)  Full local processing with WhisperX + pyannote (slow, no API costs)
      --openai   OpenAI API only, no speaker diarization (fast, no local resources)
      --hybrid   OpenAI transcription + local pyannote diarization (recommended balance)
    """
    db = ctx.obj["db"]

    if use_openai:
        # Use OpenAI API (no diarization)
        _transcribe_openai(audio_file, output, output_format, language, model)
        return

    if use_hybrid:
        # Hybrid mode: OpenAI transcription + local pyannote diarization
        _transcribe_hybrid(
            ctx, db, audio_file, language, min_speakers, max_speakers,
            threshold, no_match, output, output_format, device, model
        )
        return

    # Full local mode: WhisperX + pyannote
    from .diarizer import Diarizer
    from .diarizer import detect_device as detect_diarizer_device
    from .speaker_matcher import (
        SpeakerMatcher,
        format_match_result,
        update_segments_with_matches,
    )
    from .transcriber import Transcriber
    from .transcriber import detect_device as detect_transcriber_device

    # Detect devices (transcriber and diarizer may use different devices)
    # ctranslate2 (whisperx) only supports CPU/CUDA, not MPS
    # pyannote diarizer supports MPS
    if device == "auto":
        transcriber_device = detect_transcriber_device()
        diarizer_device = detect_diarizer_device()
    else:
        transcriber_device = "cpu" if device == "mps" else device
        diarizer_device = device

    click.echo(f"Using device: transcription={transcriber_device}, diarization={diarizer_device}")

    # Initialize components
    click.echo("Loading models...")
    transcriber = Transcriber(model_name=model, device=transcriber_device)
    diarizer = Diarizer(device=diarizer_device)
    matcher = SpeakerMatcher(db, threshold=threshold)

    # Step 1: Transcribe
    click.echo("Transcribing audio (local)...")
    aligned_result, detected_lang = transcriber.transcribe_and_align(
        audio_file,
        language=language,
    )
    click.echo(f"Detected language: {detected_lang}")

    # Step 2: Diarize
    click.echo("Performing speaker diarization...")
    diarize_result = diarizer.diarize(
        audio_file,
        min_speakers=min_speakers,
        max_speakers=max_speakers,
    )

    # Step 3: Assign speakers to segments
    segments = diarizer.assign_speakers(diarize_result, aligned_result)
    unique_speakers = diarizer.get_unique_speakers(segments)
    click.echo(f"Found {len(unique_speakers)} speaker(s): {', '.join(unique_speakers)}")

    # Step 4: Match speakers (if enabled)
    matches = []
    if not no_match:
        click.echo("Matching speakers to known profiles...")
        speaker_embeddings = {}
        for speaker_label in unique_speakers:
            # Get one embedding per speaker for matching
            embeddings = diarizer.extract_embeddings_for_speaker(
                audio_file, segments, speaker_label, max_embeddings=1
            )
            if embeddings:
                speaker_embeddings[speaker_label] = embeddings[0][0]

        if speaker_embeddings:
            matches = matcher.match_speakers(speaker_embeddings)
            for match in matches:
                click.echo(f"  {format_match_result(match)}")

            # Update segments with matched names
            segments = update_segments_with_matches(segments, matches)

    # Step 5: Store in database
    click.echo("Saving to database...")
    audio_record = db.add_audio_file(audio_file)
    transcript_record = db.add_transcript(
        audio_id=audio_record.id,
        model_name=model,
        language=detected_lang,
    )

    # Store segments
    for idx, seg in enumerate(segments):
        # Find matched speaker ID if any
        speaker_id = None
        confidence = None
        for match in matches:
            if match.matched_speaker and match.speaker_label in seg.speaker_label:
                speaker_id = match.matched_speaker.id
                confidence = match.confidence
                break

        db.add_segment(
            transcript_id=transcript_record.id,
            segment_index=idx,
            start_time=seg.start,
            end_time=seg.end,
            text=seg.text,
            speaker_label=seg.speaker_label,
            speaker_id=speaker_id,
            confidence=confidence,
        )

    click.echo(f"Transcript ID: {transcript_record.id}")

    # Step 6: Output
    output_text = _format_output(segments, output_format)

    if output:
        Path(output).write_text(output_text)
        click.echo(f"Saved to: {output}")
    else:
        click.echo("\n--- Transcript ---")
        click.echo(output_text)


def _transcribe_openai(
    audio_file: str,
    output: Optional[str],
    output_format: str,
    language: Optional[str],
    model: str,
) -> None:
    """Transcribe using OpenAI API (no diarization)."""
    import openai

    load_transcribe_env()
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        click.echo("Error: OPENAI_API_KEY not set", err=True)
        sys.exit(1)

    openai.api_key = api_key
    click.echo("Transcribing with OpenAI API (no speaker diarization)...")

    # Map model name to OpenAI model
    openai_model = "whisper-1"
    if "gpt-4o" in model.lower():
        openai_model = model

    with open(audio_file, "rb") as f:
        result = openai.audio.transcriptions.create(
            model=openai_model,
            file=f,
            language=language,
        )

    text = result.text if hasattr(result, "text") else result.get("text", "")

    if output:
        Path(output).write_text(text)
        click.echo(f"Saved to: {output}")
    else:
        click.echo("\n--- Transcript ---")
        click.echo(text)


def _transcribe_hybrid(
    ctx: click.Context,
    db,
    audio_file: str,
    language: Optional[str],
    min_speakers: Optional[int],
    max_speakers: Optional[int],
    threshold: float,
    no_match: bool,
    output: Optional[str],
    output_format: str,
    device: str,
    model: str,
) -> None:
    """Hybrid mode: OpenAI transcription + local pyannote diarization."""
    import openai

    from .diarizer import Diarizer
    from .diarizer import detect_device as detect_diarizer_device
    from .speaker_matcher import (
        SpeakerMatcher,
        format_match_result,
        update_segments_with_matches,
    )

    # Setup OpenAI
    load_transcribe_env()
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        click.echo("Error: OPENAI_API_KEY not set", err=True)
        sys.exit(1)
    openai.api_key = api_key

    # Detect diarizer device
    if device == "auto":
        diarizer_device = detect_diarizer_device()
    else:
        diarizer_device = device
    click.echo(f"Using device: transcription=OpenAI API, diarization={diarizer_device}")

    # Step 1: Transcribe with OpenAI (with timestamps)
    openai_model = "whisper-1"
    if "gpt-4o" in model.lower():
        openai_model = model

    # Check file size and chunk if needed (OpenAI limit is 25MB)
    file_size = os.path.getsize(audio_file)
    max_size = 24 * 1024 * 1024  # 24MB to be safe

    if file_size > max_size:
        click.echo(f"File size ({file_size / 1024 / 1024:.1f}MB) exceeds OpenAI limit, chunking...")
        openai_segments, detected_lang = _transcribe_openai_chunked(
            audio_file, openai_model, language
        )
    else:
        click.echo("Transcribing with OpenAI API...")
        with open(audio_file, "rb") as f:
            result = openai.audio.transcriptions.create(
                model=openai_model,
                file=f,
                language=language,
                response_format="verbose_json",
                timestamp_granularities=["segment", "word"],
            )

        # Extract segments from OpenAI response
        detected_lang = result.language if hasattr(result, "language") else language or "en"

        # Convert OpenAI segments to our format for diarization alignment
        openai_segments = []
        if hasattr(result, "segments") and result.segments:
            for seg in result.segments:
                openai_segments.append({
                    "start": seg.get("start", seg["start"]) if isinstance(seg, dict) else seg.start,
                    "end": seg.get("end", seg["end"]) if isinstance(seg, dict) else seg.end,
                    "text": seg.get("text", seg["text"]) if isinstance(seg, dict) else seg.text,
                })
        else:
            # Fallback: single segment with full text
            click.echo("Warning: No segment timestamps from OpenAI, using full text")
            openai_segments.append({
                "start": 0.0,
                "end": 0.0,
                "text": result.text if hasattr(result, "text") else str(result),
            })

    click.echo(f"Detected language: {detected_lang}")
    click.echo(f"Got {len(openai_segments)} segments from OpenAI")

    # Step 2: Diarize locally with pyannote
    click.echo("Loading diarization model...")
    diarizer = Diarizer(device=diarizer_device)
    matcher = SpeakerMatcher(db, threshold=threshold)

    click.echo("Performing speaker diarization...")
    diarize_result = diarizer.diarize(
        audio_file,
        min_speakers=min_speakers,
        max_speakers=max_speakers,
    )

    # Step 3: Merge OpenAI transcription with pyannote diarization
    click.echo("Merging transcription with speaker labels...")
    segments = _merge_transcription_with_diarization(openai_segments, diarize_result)
    unique_speakers = diarizer.get_unique_speakers(segments)
    click.echo(f"Found {len(unique_speakers)} speaker(s): {', '.join(unique_speakers)}")

    # Step 4: Match speakers (if enabled)
    matches = []
    if not no_match:
        click.echo("Matching speakers to known profiles...")
        speaker_embeddings = {}
        for speaker_label in unique_speakers:
            embeddings = diarizer.extract_embeddings_for_speaker(
                audio_file, segments, speaker_label, max_embeddings=1
            )
            if embeddings:
                speaker_embeddings[speaker_label] = embeddings[0][0]

        if speaker_embeddings:
            matches = matcher.match_speakers(speaker_embeddings)
            for match in matches:
                click.echo(f"  {format_match_result(match)}")
            segments = update_segments_with_matches(segments, matches)

    # Step 5: Store in database
    click.echo("Saving to database...")
    audio_record = db.add_audio_file(audio_file)
    transcript_record = db.add_transcript(
        audio_id=audio_record.id,
        model_name=f"openai/{openai_model}",
        language=detected_lang,
    )

    for idx, seg in enumerate(segments):
        speaker_id = None
        confidence = None
        for match in matches:
            if match.matched_speaker and match.speaker_label in seg.speaker_label:
                speaker_id = match.matched_speaker.id
                confidence = match.confidence
                break

        db.add_segment(
            transcript_id=transcript_record.id,
            segment_index=idx,
            start_time=seg.start,
            end_time=seg.end,
            text=seg.text,
            speaker_label=seg.speaker_label,
            speaker_id=speaker_id,
            confidence=confidence,
        )

    click.echo(f"Transcript ID: {transcript_record.id}")

    # Step 6: Output
    output_text = _format_output(segments, output_format)

    if output:
        Path(output).write_text(output_text)
        click.echo(f"Saved to: {output}")
    else:
        click.echo("\n--- Transcript ---")
        click.echo(output_text)


def _merge_transcription_with_diarization(
    openai_segments: list[dict],
    diarize_result,
) -> list[DiarizedSegment]:
    """Merge OpenAI transcription segments with pyannote diarization.

    Args:
        openai_segments: List of segments from OpenAI with start, end, text.
        diarize_result: Diarization result from pyannote (pandas DataFrame).

    Returns:
        List of DiarizedSegment with speaker labels assigned.
    """
    segments = []

    for seg in openai_segments:
        seg_start = seg["start"]
        seg_end = seg["end"]
        seg_text = seg["text"].strip()

        if not seg_text:
            continue

        # Find the speaker who talks most during this segment
        speaker = _find_speaker_for_segment(seg_start, seg_end, diarize_result)

        segments.append(
            DiarizedSegment(
                start=seg_start,
                end=seg_end,
                text=seg_text,
                speaker_label=speaker,
            )
        )

    return segments


def _find_speaker_for_segment(start: float, end: float, diarize_result) -> str:
    """Find the dominant speaker for a time segment.

    Args:
        start: Segment start time.
        end: Segment end time.
        diarize_result: Diarization DataFrame with start, end, speaker columns.

    Returns:
        Speaker label (e.g., "SPEAKER_00") or "UNKNOWN".
    """
    if diarize_result is None or len(diarize_result) == 0:
        return "UNKNOWN"

    # Calculate overlap with each speaker segment
    speaker_overlap = {}

    for _, row in diarize_result.iterrows():
        diar_start = row["start"]
        diar_end = row["end"]
        speaker = row["speaker"]

        # Calculate overlap
        overlap_start = max(start, diar_start)
        overlap_end = min(end, diar_end)
        overlap = max(0, overlap_end - overlap_start)

        if overlap > 0:
            speaker_overlap[speaker] = speaker_overlap.get(speaker, 0) + overlap

    if not speaker_overlap:
        return "UNKNOWN"

    # Return speaker with most overlap
    return max(speaker_overlap, key=speaker_overlap.get)


def _transcribe_openai_chunked(
    audio_file: str,
    model: str,
    language: Optional[str],
    chunk_duration_minutes: int = 10,
) -> tuple[list[dict], str]:
    """Transcribe a large audio file by chunking it.

    Args:
        audio_file: Path to audio file.
        model: OpenAI model name.
        language: Language code or None for auto-detect.
        chunk_duration_minutes: Duration of each chunk in minutes.

    Returns:
        Tuple of (segments list, detected language).
    """
    import tempfile

    import openai
    from pydub import AudioSegment

    click.echo(f"Loading audio file for chunking...")
    audio = AudioSegment.from_file(audio_file)
    total_duration_ms = len(audio)
    total_duration_sec = total_duration_ms / 1000
    click.echo(f"Audio duration: {total_duration_sec / 60:.1f} minutes")

    chunk_duration_ms = chunk_duration_minutes * 60 * 1000
    chunks = []
    start_ms = 0

    while start_ms < total_duration_ms:
        end_ms = min(start_ms + chunk_duration_ms, total_duration_ms)
        chunks.append((start_ms, end_ms))
        start_ms = end_ms

    click.echo(f"Split into {len(chunks)} chunks of ~{chunk_duration_minutes} minutes each")

    all_segments = []
    detected_lang = language or "en"

    for i, (start_ms, end_ms) in enumerate(chunks):
        chunk_start_sec = start_ms / 1000
        click.echo(f"Transcribing chunk {i + 1}/{len(chunks)} ({start_ms / 1000 / 60:.1f}-{end_ms / 1000 / 60:.1f} min)...")

        # Extract chunk
        chunk_audio = audio[start_ms:end_ms]

        # Export to temp file
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp:
            tmp_path = tmp.name
            chunk_audio.export(tmp_path, format="mp3")

        try:
            # Transcribe chunk
            with open(tmp_path, "rb") as f:
                result = openai.audio.transcriptions.create(
                    model=model,
                    file=f,
                    language=language,
                    response_format="verbose_json",
                    timestamp_granularities=["segment", "word"],
                )

            # Update detected language from first chunk
            if i == 0 and hasattr(result, "language") and result.language:
                detected_lang = result.language

            # Extract and adjust segments
            if hasattr(result, "segments") and result.segments:
                for seg in result.segments:
                    seg_start = seg.get("start", seg["start"]) if isinstance(seg, dict) else seg.start
                    seg_end = seg.get("end", seg["end"]) if isinstance(seg, dict) else seg.end
                    seg_text = seg.get("text", seg["text"]) if isinstance(seg, dict) else seg.text

                    # Adjust timestamps to global timeline
                    all_segments.append({
                        "start": seg_start + chunk_start_sec,
                        "end": seg_end + chunk_start_sec,
                        "text": seg_text,
                    })
        finally:
            # Clean up temp file
            os.unlink(tmp_path)

    return all_segments, detected_lang


def _format_output(segments: list[DiarizedSegment], format: str) -> str:
    """Format segments for output."""
    from .transcriber import (
        segments_to_srt,
        segments_to_text,
        segments_to_vtt,
    )

    if format == "json":
        data = [
            {
                "start": seg.start,
                "end": seg.end,
                "speaker": seg.speaker_label,
                "text": seg.text,
            }
            for seg in segments
        ]
        return json.dumps(data, indent=2)
    elif format == "srt":
        return segments_to_srt(segments)
    elif format == "vtt":
        return segments_to_vtt(segments)
    else:  # txt
        return segments_to_text(segments)


# Speaker management commands


@cli.group()
def speakers() -> None:
    """Manage speaker profiles."""
    pass


@speakers.command("list")
@click.option("-v", "--verbose", is_flag=True, help="Show detailed info")
@click.pass_context
def speakers_list(ctx: click.Context, verbose: bool) -> None:
    """List all known speakers."""
    db = ctx.obj["db"]
    speakers_list = db.list_speakers()

    if not speakers_list:
        click.echo("No speakers found.")
        return

    for speaker in speakers_list:
        if verbose:
            click.echo(f"{speaker.name}")
            click.echo(f"  ID: {speaker.id}")
            click.echo(f"  Samples: {speaker.sample_count}")
            click.echo(f"  Created: {format_local_time(speaker.created_at)}")
            click.echo(f"  Updated: {format_local_time(speaker.updated_at)}")
            if speaker.notes:
                click.echo(f"  Notes: {speaker.notes}")
        else:
            click.echo(f"{speaker.name} ({speaker.sample_count} samples)")


@speakers.command("add")
@click.argument("name")
@click.option("--notes", default=None, help="Notes about the speaker")
@click.pass_context
def speakers_add(ctx: click.Context, name: str, notes: Optional[str]) -> None:
    """Add a new speaker profile."""
    db = ctx.obj["db"]

    existing = db.get_speaker(name)
    if existing:
        click.echo(f"Error: Speaker '{name}' already exists.", err=True)
        sys.exit(1)

    speaker = db.add_speaker(name, notes)
    click.echo(f"Added speaker: {speaker.name} (ID: {speaker.id})")


@speakers.command("delete")
@click.argument("name")
@click.option("-f", "--force", is_flag=True, help="Skip confirmation")
@click.pass_context
def speakers_delete(ctx: click.Context, name: str, force: bool) -> None:
    """Delete a speaker profile."""
    db = ctx.obj["db"]

    speaker = db.get_speaker(name)
    if not speaker:
        click.echo(f"Error: Speaker '{name}' not found.", err=True)
        sys.exit(1)

    if not force:
        if not click.confirm(f"Delete speaker '{name}' and all their voice samples?"):
            click.echo("Cancelled.")
            return

    db.delete_speaker(name)
    click.echo(f"Deleted speaker: {name}")


@speakers.command("rename")
@click.argument("old_name")
@click.argument("new_name")
@click.pass_context
def speakers_rename(ctx: click.Context, old_name: str, new_name: str) -> None:
    """Rename a speaker."""
    db = ctx.obj["db"]

    if not db.get_speaker(old_name):
        click.echo(f"Error: Speaker '{old_name}' not found.", err=True)
        sys.exit(1)

    if db.get_speaker(new_name):
        click.echo(f"Error: Speaker '{new_name}' already exists.", err=True)
        sys.exit(1)

    db.rename_speaker(old_name, new_name)
    click.echo(f"Renamed: {old_name} -> {new_name}")


@speakers.command("info")
@click.argument("name")
@click.pass_context
def speakers_info(ctx: click.Context, name: str) -> None:
    """Show detailed info about a speaker."""
    db = ctx.obj["db"]

    speaker = db.get_speaker(name)
    if not speaker:
        click.echo(f"Error: Speaker '{name}' not found.", err=True)
        sys.exit(1)

    click.echo(f"Name: {speaker.name}")
    click.echo(f"ID: {speaker.id}")
    click.echo(f"Voice samples: {speaker.sample_count}")
    click.echo(f"Created: {format_local_time(speaker.created_at)}")
    click.echo(f"Updated: {format_local_time(speaker.updated_at)}")
    if speaker.notes:
        click.echo(f"Notes: {speaker.notes}")


# Transcript management commands


@cli.group()
def transcripts() -> None:
    """Manage transcripts."""
    pass


@transcripts.command("list")
@click.pass_context
def transcripts_list(ctx: click.Context) -> None:
    """List all transcripts."""
    db = ctx.obj["db"]
    transcripts = db.list_transcripts()

    if not transcripts:
        click.echo("No transcripts found.")
        return

    for transcript, audio in transcripts:
        filename = Path(audio.file_path).name
        duration = f"{audio.duration_seconds:.1f}s" if audio.duration_seconds else "?"
        click.echo(f"[{transcript.id}] {filename} ({duration}) - {format_local_time(transcript.created_at)}")


@transcripts.command("show")
@click.argument("transcript_id", type=int)
@click.option("--format", "output_format", type=click.Choice(["txt", "json", "srt", "vtt"]), default="txt")
@click.pass_context
def transcripts_show(ctx: click.Context, transcript_id: int, output_format: str) -> None:
    """Show a transcript."""
    db = ctx.obj["db"]

    transcript = db.get_transcript(transcript_id)
    if not transcript:
        click.echo(f"Error: Transcript {transcript_id} not found.", err=True)
        sys.exit(1)

    segments = db.get_segments(transcript_id)
    if not segments:
        click.echo("No segments found.")
        return

    # Convert to DiarizedSegment for formatting
    diarized = [
        DiarizedSegment(
            start=seg.start_time,
            end=seg.end_time,
            text=seg.text,
            speaker_label=seg.speaker_label or "UNKNOWN",
        )
        for seg in segments
    ]

    output = _format_output(diarized, output_format)
    click.echo(output)


@transcripts.command("update")
@click.argument("transcript_id", type=int)
@click.option("--assign", multiple=True, help="Assign speaker: SPEAKER_00=Name")
@click.pass_context
def transcripts_update(ctx: click.Context, transcript_id: int, assign: tuple) -> None:
    """Update a transcript.

    Assign speakers and learn their voices.

    Example: transcripts update 1 --assign "SPEAKER_00=John" --assign "SPEAKER_01=Jane"
    """
    db = ctx.obj["db"]

    transcript = db.get_transcript(transcript_id)
    if not transcript:
        click.echo(f"Error: Transcript {transcript_id} not found.", err=True)
        sys.exit(1)

    if not assign:
        click.echo("No assignments provided. Use --assign 'SPEAKER_XX=Name'")
        return

    # Import heavy dependencies only when needed
    from .diarizer import Diarizer
    from .speaker_matcher import SpeakerMatcher

    # Get audio file for embedding extraction
    transcripts_data = db.list_transcripts()
    audio_path = None
    audio_id = None
    for t, a in transcripts_data:
        if t.id == transcript_id:
            audio_path = a.file_path
            audio_id = a.id
            break

    if not audio_path or not Path(audio_path).exists():
        click.echo("Warning: Audio file not found, cannot learn voice embeddings.")
        audio_path = None

    diarizer = None
    matcher = None
    if audio_path:
        diarizer = Diarizer()
        matcher = SpeakerMatcher(db)

    for assignment in assign:
        if "=" not in assignment:
            click.echo(f"Invalid assignment: {assignment}. Use format SPEAKER_XX=Name", err=True)
            continue

        label, name = assignment.split("=", 1)
        name = name.strip()
        label = label.strip()

        # Get or create speaker
        speaker = db.get_speaker(name)
        if not speaker:
            speaker = db.add_speaker(name)
            click.echo(f"Created speaker: {name}")

        # Assign speaker to segments
        updated = db.assign_speaker_to_label(transcript_id, label, speaker.id)
        click.echo(f"Assigned {label} -> {name} ({updated} segments)")

        # Learn voice embeddings
        if audio_path and diarizer and matcher:
            segments = db.get_segments_by_speaker_label(transcript_id, label)
            if segments:
                # Convert to DiarizedSegment for embedding extraction
                diarized = [
                    DiarizedSegment(
                        start=seg.start_time,
                        end=seg.end_time,
                        text=seg.text,
                        speaker_label=seg.speaker_label or label,
                    )
                    for seg in segments
                ]

                embeddings = diarizer.extract_embeddings_for_speaker(
                    audio_path, diarized, label, max_embeddings=5
                )

                if embeddings:
                    count = matcher.learn_speaker(speaker.id, embeddings, audio_id)
                    click.echo(f"  Learned {count} voice sample(s) for {name}")


@transcripts.command("delete")
@click.argument("transcript_id", type=int)
@click.option("-f", "--force", is_flag=True, help="Skip confirmation")
@click.pass_context
def transcripts_delete(ctx: click.Context, transcript_id: int, force: bool) -> None:
    """Delete a transcript."""
    db = ctx.obj["db"]

    transcript = db.get_transcript(transcript_id)
    if not transcript:
        click.echo(f"Error: Transcript {transcript_id} not found.", err=True)
        sys.exit(1)

    if not force:
        if not click.confirm(f"Delete transcript {transcript_id}?"):
            click.echo("Cancelled.")
            return

    db.delete_transcript(transcript_id)
    click.echo(f"Deleted transcript: {transcript_id}")


# Export command


@cli.command()
@click.argument("transcript_id", type=int)
@click.option("-f", "--format", "output_format", type=click.Choice(["txt", "json", "srt", "vtt"]), default="txt")
@click.option("-o", "--output", type=click.Path(), default=None, help="Output file path")
@click.option("--include-timestamps/--no-timestamps", default=True)
@click.pass_context
def export(
    ctx: click.Context,
    transcript_id: int,
    output_format: str,
    output: Optional[str],
    include_timestamps: bool,
) -> None:
    """Export a transcript to file."""
    db = ctx.obj["db"]

    transcript = db.get_transcript(transcript_id)
    if not transcript:
        click.echo(f"Error: Transcript {transcript_id} not found.", err=True)
        sys.exit(1)

    segments = db.get_segments(transcript_id)
    if not segments:
        click.echo("No segments found.")
        return

    # Convert to DiarizedSegment
    diarized = [
        DiarizedSegment(
            start=seg.start_time,
            end=seg.end_time,
            text=seg.text,
            speaker_label=seg.speaker_label or "UNKNOWN",
        )
        for seg in segments
    ]

    output_text = _format_output(diarized, output_format)

    if output:
        Path(output).write_text(output_text)
        click.echo(f"Exported to: {output}")
    else:
        click.echo(output_text)


# Web server command


@cli.command()
@click.option("--host", default="0.0.0.0", help="Host to bind to")
@click.option("--port", "-p", default=8000, help="Port to bind to")
@click.option("--reload", is_flag=True, help="Enable auto-reload for development")
def serve(host: str, port: int, reload: bool) -> None:
    """Start the web UI server.

    Example: transcribe-local serve --port 8080
    """
    import uvicorn

    click.echo(f"Starting web server at http://{host}:{port}")
    click.echo("Press Ctrl+C to stop")

    uvicorn.run(
        "transcribe_local.web.app:app",
        host=host,
        port=port,
        reload=reload,
    )


# Recording commands


@cli.group()
def record() -> None:
    """Continuous audio recording and VAD filtering."""
    pass


@record.command("start")
@click.option("--output-dir", "-o", type=click.Path(), default=None, help="Output directory for recordings")
@click.option("--duration", "-d", type=int, default=600, help="Chunk duration in seconds (default: 600 = 10 min)")
@click.option("--device", help="Audio input device (platform-specific)")
@click.option("--background", "-b", is_flag=True, help="Run in background")
def record_start(output_dir: Optional[str], duration: int, device: Optional[str], background: bool) -> None:
    """Start continuous audio recording.

    Records audio in chunks to the output directory.
    """
    from .recorder.daemon import RecordingDaemon, DEFAULT_OUTPUT_DIR

    output_path = Path(output_dir) if output_dir else DEFAULT_OUTPUT_DIR

    # Check if already running
    pid = RecordingDaemon.is_running(output_path)
    if pid:
        click.echo(f"Recording daemon is already running (PID: {pid})")
        return

    if background:
        # Fork to background
        import subprocess
        cmd = [sys.executable, "-m", "transcribe_local.recorder.daemon", "start"]
        if output_dir:
            cmd.extend(["--output-dir", output_dir])
        cmd.extend(["--duration", str(duration)])
        if device:
            cmd.extend(["--device", device])

        subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True,
        )
        click.echo("Recording daemon started in background")
        click.echo(f"Output directory: {output_path}")
    else:
        daemon = RecordingDaemon(
            output_dir=output_path,
            chunk_duration=duration,
            input_device=device,
        )
        daemon.start()


@record.command("stop")
@click.option("--output-dir", "-o", type=click.Path(), default=None, help="Output directory for recordings")
def record_stop(output_dir: Optional[str]) -> None:
    """Stop the recording daemon."""
    from .recorder.daemon import RecordingDaemon, DEFAULT_OUTPUT_DIR

    output_path = Path(output_dir) if output_dir else DEFAULT_OUTPUT_DIR

    if RecordingDaemon.stop_running(output_path):
        click.echo("Recording daemon stopped")
    else:
        click.echo("Recording daemon is not running")


@record.command("status")
@click.option("--output-dir", "-o", type=click.Path(), default=None, help="Output directory for recordings")
def record_status(output_dir: Optional[str]) -> None:
    """Check recording daemon status."""
    from .recorder.daemon import RecordingDaemon, DEFAULT_OUTPUT_DIR
    from .recorder.vad_filter import VADFilter

    output_path = Path(output_dir) if output_dir else DEFAULT_OUTPUT_DIR

    # Check recorder
    recorder_pid = RecordingDaemon.is_running(output_path)
    if recorder_pid:
        click.echo(f"Recording daemon: running (PID: {recorder_pid})")
    else:
        click.echo("Recording daemon: stopped")

    # Check VAD filter
    vad_pid = VADFilter.is_running(output_path)
    if vad_pid:
        click.echo(f"VAD filter: running (PID: {vad_pid})")
    else:
        click.echo("VAD filter: stopped")

    # Show folder stats
    if output_path.exists():
        recordings = list(output_path.glob("recording-*.mp3"))
        click.echo(f"Recordings folder: {output_path}")
        click.echo(f"Pending files: {len(recordings)}")


@record.command("devices")
def record_devices() -> None:
    """List available audio input devices."""
    from .recorder.daemon import list_audio_devices
    list_audio_devices()


# VAD filter commands


@cli.group()
def vad() -> None:
    """Voice Activity Detection filter for recordings."""
    pass


@vad.command("start")
@click.option("--recordings-dir", "-r", type=click.Path(), default=None, help="Recordings input directory")
@click.option("--ready-dir", "-o", type=click.Path(), default=None, help="Output directory for files with speech")
@click.option("--min-speech", "-m", type=float, default=0.05, help="Minimum speech ratio to keep (default: 0.05)")
@click.option("--interval", "-i", type=int, default=30, help="Check interval in seconds (default: 30)")
@click.option("--engine", type=click.Choice(["silero", "webrtc"]), default="silero", help="VAD engine")
@click.option("--background", "-b", is_flag=True, help="Run in background")
def vad_start(
    recordings_dir: Optional[str],
    ready_dir: Optional[str],
    min_speech: float,
    interval: int,
    engine: str,
    background: bool,
) -> None:
    """Start VAD filter to process recordings.

    Monitors the recordings folder and moves files with speech to the ready folder.
    Files without speech are deleted.
    """
    from .recorder.vad_filter import VADFilter, DEFAULT_RECORDINGS_DIR, DEFAULT_READY_DIR

    recordings_path = Path(recordings_dir) if recordings_dir else DEFAULT_RECORDINGS_DIR
    ready_path = Path(ready_dir) if ready_dir else DEFAULT_READY_DIR

    # Check if already running
    pid = VADFilter.is_running(recordings_path)
    if pid:
        click.echo(f"VAD filter is already running (PID: {pid})")
        return

    if background:
        import subprocess
        cmd = [sys.executable, "-m", "transcribe_local.recorder.vad_filter", "start"]
        if recordings_dir:
            cmd.extend(["--recordings-dir", recordings_dir])
        if ready_dir:
            cmd.extend(["--ready-dir", ready_dir])
        cmd.extend(["--min-speech", str(min_speech)])
        cmd.extend(["--interval", str(interval)])
        cmd.extend(["--engine", engine])

        subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True,
        )
        click.echo("VAD filter started in background")
        click.echo(f"Monitoring: {recordings_path}")
        click.echo(f"Ready folder: {ready_path}")
    else:
        vad_filter = VADFilter(
            recordings_dir=recordings_path,
            ready_dir=ready_path,
            min_speech_ratio=min_speech,
            check_interval=interval,
            vad_engine=engine,
        )
        vad_filter.start()


@vad.command("stop")
@click.option("--recordings-dir", "-r", type=click.Path(), default=None, help="Recordings input directory")
def vad_stop(recordings_dir: Optional[str]) -> None:
    """Stop the VAD filter."""
    from .recorder.vad_filter import VADFilter, DEFAULT_RECORDINGS_DIR

    recordings_path = Path(recordings_dir) if recordings_dir else DEFAULT_RECORDINGS_DIR

    if VADFilter.stop_running(recordings_path):
        click.echo("VAD filter stopped")
    else:
        click.echo("VAD filter is not running")


@vad.command("process")
@click.option("--recordings-dir", "-r", type=click.Path(), default=None, help="Recordings input directory")
@click.option("--ready-dir", "-o", type=click.Path(), default=None, help="Output directory for files with speech")
@click.option("--min-speech", "-m", type=float, default=0.05, help="Minimum speech ratio to keep")
@click.option("--engine", type=click.Choice(["silero", "webrtc"]), default="silero", help="VAD engine")
def vad_process(
    recordings_dir: Optional[str],
    ready_dir: Optional[str],
    min_speech: float,
    engine: str,
) -> None:
    """Process recordings folder once (no continuous monitoring)."""
    from .recorder.vad_filter import VADFilter, DEFAULT_RECORDINGS_DIR, DEFAULT_READY_DIR

    recordings_path = Path(recordings_dir) if recordings_dir else DEFAULT_RECORDINGS_DIR
    ready_path = Path(ready_dir) if ready_dir else DEFAULT_READY_DIR

    vad_filter = VADFilter(
        recordings_dir=recordings_path,
        ready_dir=ready_path,
        min_speech_ratio=min_speech,
        vad_engine=engine,
    )

    results = vad_filter.process_folder()
    click.echo(f"Results: {results['kept']} kept, {results['deleted']} deleted, {results['error']} errors")


# Upload commands


@cli.group()
def upload() -> None:
    """Upload recordings to remote transcription server."""
    pass


@upload.command("start")
@click.option("--server", "-s", default=None, help="Server URL (defaults to configured server_url)")
@click.option("--ready-dir", "-r", type=click.Path(), default=None, help="Ready folder to monitor")
@click.option("--interval", "-i", type=int, default=30, help="Check interval in seconds (default: 30)")
@click.option("--mode", "-m", type=click.Choice(["hybrid", "openai", "local"]), default="hybrid", help="Transcription mode")
@click.option("--language", "-l", help="Language code (auto-detect if omitted)")
@click.option("--delete", is_flag=True, help="Delete files after upload (default: move to uploaded/)")
@click.option("--background", "-b", is_flag=True, help="Run in background")
def upload_start(
    server: Optional[str],
    ready_dir: Optional[str],
    interval: int,
    mode: str,
    language: Optional[str],
    delete: bool,
    background: bool,
) -> None:
    """Start uploader to send recordings to remote server.

    Example: transcribe-local upload start --server http://192.168.68.111:8000
    """
    from .recorder.uploader import Uploader, DEFAULT_READY_DIR

    server_url = (server or get_settings().server_url or "").strip()
    if not server_url:
        click.echo("Error: No server URL configured. Run `transcribe-local setup` or pass --server.", err=True)
        sys.exit(1)

    ready_path = Path(ready_dir) if ready_dir else DEFAULT_READY_DIR

    # Check if already running
    pid = Uploader.is_running(ready_path)
    if pid:
        click.echo(f"Uploader is already running (PID: {pid})")
        return

    if background:
        import subprocess
        cmd = [sys.executable, "-m", "transcribe_local.recorder.uploader", "start"]
        cmd.extend(["--server", server_url])
        if ready_dir:
            cmd.extend(["--ready-dir", ready_dir])
        cmd.extend(["--interval", str(interval)])
        cmd.extend(["--mode", mode])
        if language:
            cmd.extend(["--language", language])
        if delete:
            cmd.append("--delete")

        subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True,
        )
        click.echo("Uploader started in background")
        click.echo(f"Server: {server_url}")
        click.echo(f"Ready folder: {ready_path}")
    else:
        uploader = Uploader(
            server_url=server_url,
            ready_dir=ready_path,
            check_interval=interval,
            mode=mode,
            language=language,
            delete_after_upload=delete,
        )
        uploader.start()


@upload.command("stop")
@click.option("--ready-dir", "-r", type=click.Path(), default=None, help="Ready folder")
def upload_stop(ready_dir: Optional[str]) -> None:
    """Stop the uploader."""
    from .recorder.uploader import Uploader, DEFAULT_READY_DIR

    ready_path = Path(ready_dir) if ready_dir else DEFAULT_READY_DIR

    if Uploader.stop_running(ready_path):
        click.echo("Uploader stopped")
    else:
        click.echo("Uploader is not running")


@upload.command("status")
@click.option("--ready-dir", "-r", type=click.Path(), default=None, help="Ready folder")
def upload_status(ready_dir: Optional[str]) -> None:
    """Check uploader status."""
    from .recorder.uploader import Uploader, DEFAULT_READY_DIR, DEFAULT_UPLOADED_DIR

    ready_path = Path(ready_dir) if ready_dir else DEFAULT_READY_DIR
    uploaded_path = DEFAULT_UPLOADED_DIR
    audio_extensions = {".mp3", ".wav", ".m4a", ".mp4", ".ogg", ".flac", ".aac", ".webm", ".opus"}

    pid = Uploader.is_running(ready_path)
    if pid:
        click.echo(f"Uploader: running (PID: {pid})")
    else:
        click.echo("Uploader: stopped")

    # Show folder stats
    if ready_path.exists():
        ready_count = len([f for f in ready_path.iterdir() if f.suffix.lower() in audio_extensions])
        click.echo(f"Ready folder: {ready_path} ({ready_count} files)")
    if uploaded_path.exists():
        uploaded_count = len([f for f in uploaded_path.iterdir() if f.suffix.lower() in audio_extensions])
        click.echo(f"Uploaded folder: {uploaded_path} ({uploaded_count} files)")


@upload.command("once")
@click.option("--server", "-s", default=None, help="Server URL (defaults to configured server_url)")
@click.option("--ready-dir", "-r", type=click.Path(), default=None, help="Ready folder")
@click.option("--mode", "-m", type=click.Choice(["hybrid", "openai", "local"]), default="hybrid")
@click.option("--language", "-l", help="Language code")
@click.option("--delete", is_flag=True, help="Delete files after upload")
def upload_once(
    server: Optional[str],
    ready_dir: Optional[str],
    mode: str,
    language: Optional[str],
    delete: bool,
) -> None:
    """Upload all ready files once (no continuous monitoring)."""
    from .recorder.uploader import Uploader, DEFAULT_READY_DIR

    server_url = (server or get_settings().server_url or "").strip()
    if not server_url:
        click.echo("Error: No server URL configured. Run `transcribe-local setup` or pass --server.", err=True)
        sys.exit(1)

    ready_path = Path(ready_dir) if ready_dir else DEFAULT_READY_DIR

    uploader = Uploader(
        server_url=server_url,
        ready_dir=ready_path,
        mode=mode,
        language=language,
        delete_after_upload=delete,
    )

    results = uploader.process_folder()
    click.echo(f"Results: {results['uploaded']} uploaded, {results['failed']} failed")


# ============== Jobs Command Group ==============


@cli.group()
def jobs():
    """Manage the job runner daemon."""
    pass


@jobs.command("start")
@click.option("--mode", "-m", type=click.Choice(["hybrid", "openai", "local"]), help="Transcription mode")
@click.option("--language", "-l", help="Language code (default: auto-detect)")
@click.option("--interval", "-i", type=int, default=30, help="Check interval in seconds")
def jobs_start(mode: Optional[str], language: Optional[str], interval: int) -> None:
    """Start the job runner daemon."""
    from .job_runner import JobRunner

    pid = JobRunner.is_running()
    if pid:
        click.echo(f"Job runner is already running (PID: {pid})")
        return

    runner = JobRunner(
        mode=mode,
        language=language,
        check_interval=interval,
    )
    runner.start()


@jobs.command("stop")
def jobs_stop() -> None:
    """Stop the job runner daemon."""
    from .job_runner import JobRunner

    if JobRunner.stop_running():
        click.echo("Job runner stopped")
    else:
        click.echo("Job runner is not running")


@jobs.command("status")
def jobs_status() -> None:
    """Check job runner status."""
    from .job_runner import JobRunner
    from .web.config import get_settings

    settings = get_settings()
    pid = JobRunner.is_running()

    if pid:
        click.echo(f"Job runner: running (PID: {pid})")
    else:
        click.echo("Job runner: stopped")

    # Show folder stats
    uploads_dir = settings.upload_dir
    processed_dir = settings.data_dir / "processed"

    if uploads_dir.exists():
        audio_extensions = {".mp3", ".wav", ".m4a", ".mp4", ".ogg", ".flac", ".aac", ".webm", ".opus"}
        unprocessed = [f for f in uploads_dir.iterdir() if f.suffix.lower() in audio_extensions]
        click.echo(f"Uploads folder: {uploads_dir} ({len(unprocessed)} files)")

    if processed_dir.exists():
        processed = [f for f in processed_dir.iterdir() if f.suffix.lower() in audio_extensions]
        click.echo(f"Processed folder: {processed_dir} ({len(processed)} files)")


@jobs.command("process")
@click.option("--mode", "-m", type=click.Choice(["hybrid", "openai", "local"]), help="Transcription mode")
@click.option("--language", "-l", help="Language code (default: auto-detect)")
def jobs_process(mode: Optional[str], language: Optional[str]) -> None:
    """Process all pending files once (no continuous monitoring)."""
    from .job_runner import JobRunner

    runner = JobRunner(
        mode=mode,
        language=language,
    )
    results = runner.process_backlog()
    click.echo(f"\nResults: {results['processed']} processed, {results['failed']} failed, {results['skipped']} skipped")


def main():
    """Entry point."""
    cli(obj={})


if __name__ == "__main__":
    main()
